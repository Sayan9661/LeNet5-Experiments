# LeNet5-Experiments: Effects of Batch Normalization and dropout on GoogleLeNet-5 convolutional neural network

This was done as part of High Performance Machine Learning course.<br>

## Motivation and Background
Batch Norm and Dropout are standard techniques used to prevent a model from over-fitting data but it is not always clear as to what technique is preferred and  what their effects in conjunction might be.<br>

## Task
Trained LeNet-5 while varying Batch Norm and Dropout to see their effects on test and validation accuracy.

## Dataset
MNIST

## Requirements
Jupyter notebook. <br>
Need Numpy and Pytorch.


### Miscellaneous Topics
There were some other tasks about parameter cost calculation of VGG, AlexNet and Inception module.


