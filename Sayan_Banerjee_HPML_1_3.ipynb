{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3930e8db",
   "metadata": {},
   "source": [
    "# HPML LAB 3\n",
    "### Sayan Banerjee\n",
    "### sb7594"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cead7745",
   "metadata": {},
   "source": [
    "## Problem 1 - *Batch Normalization, Dropout, MNIST* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0598e93e",
   "metadata": {},
   "source": [
    "Batch normalization and Dropout are used as effective regularization techniques. However, it is not clear\n",
    "which one should be preferred and whether their benefits add up when used in conjunction. In this problem,\n",
    "we will compare batch normalization, dropout, and their conjunction using MNIST and LeNet-5 (see e.g.,\n",
    "http://yann.lecun.com/exdb/lenet/). LeNet-5 is one of the earliest convolutional neural networks developed\n",
    "for image classification and its implementation in all major frameworks is available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02781081",
   "metadata": {},
   "source": [
    "1. Explain the terms co-adaptation and internal covariance-shift. Use examples if needed. You may need\n",
    "to refer to two papers mentioned below to answer this question. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0d103e",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "### Co-adaptation\n",
    "In a neural network sometimes hidden units(neurons) might become highly correlated. This is called co-adaptation. This happens when a unit changes in a particular way to fix mistakes of other neurons. This leads to over-fitting as it performs well on train data but does not geralise well in test data. The way to fix this problem is called dropout.<br>\n",
    "As stated in the paper \" Dropout: A Simple Way to Prevent Neural Networks from Overfitting.\":<br>\n",
    "<br>\n",
    "\"In a standard neural network, the derivative received by each parameter tells it how it should change so the final loss function is reduced, given what all other units are doing.Therefore, units may change in a way that they fix up the mistakes of the other units.This may lead to complex co-adaptations. This in turn leads to overfitting because these co-adaptations do not generalize to unseen data. We hypothesize that for each hidden unit, dropout prevents co-adaptation by making the presence of other hidden units unreliable. Therefore, a hidden unit cannot rely on other specific units to correct its mistakes. It must perform well in a wide variety of different contexts provided by the other hidden units. To observe this effect directly, we look at the first level features learned by neural networks trained on visual tasks with and without dropout.\"\n",
    "\n",
    "### Internal Covariate Shift\n",
    "\n",
    "In the paper \"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.\" Internal covariate shift is defined as:<br>\n",
    "\n",
    "\"We define Internal Covariate Shift as the change in the\n",
    "distribution of network activations due to the change in\n",
    "network parameters during training\"\n",
    "\n",
    "\n",
    "This as the paper mentions can be fixed with batch nonrmalization.<br>\n",
    "As stated by:<br>\n",
    "\"Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02c75ae",
   "metadata": {},
   "source": [
    "2. Batch normalization is traditionally used in hidden layers, for the input layer standard normalization\n",
    "is used. In standard normalization, the mean and standard deviation are calculated using the entire\n",
    "training dataset whereas in batch normalization these statistics are calculated for each mini-batch.\n",
    "Train LeNet-5 with standard normalization of input and batch normalization for hidden layers. What\n",
    "are the learned batch norm parameters for each layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d93a32",
   "metadata": {},
   "source": [
    "Answer:\n",
    "Below.\n",
    "The learned parameters for batch norm are the alpha and beta values , that is alpha for scale and beta for shift. This is to normalize the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6283a853",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc25f74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.datasets import mnist\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b95fb77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model, data_loader, device):\n",
    "    correct_pred = 0 \n",
    "    n = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for X, y_true in data_loader:\n",
    "\n",
    "            X = X.to(device)\n",
    "            y_true = y_true.to(device)\n",
    "\n",
    "            _, y_prob = model(X)\n",
    "            _, predicted_labels = torch.max(y_prob, 1)\n",
    "\n",
    "            n += y_true.size(0)\n",
    "            correct_pred += (predicted_labels == y_true).sum()\n",
    "\n",
    "    return correct_pred.float() / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57f694d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = mnist.MNIST(root='./train', train=True, transform=transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((32, 32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = (0.1307,), std = (0.3081,))\n",
    "    ]),download=True)\n",
    "test_dataset = mnist.MNIST(root='./test', train=False, transform=transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((32, 32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = (0.1307,), std = (0.3081,))\n",
    "    ]),download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=256)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256)\n",
    "\n",
    "\n",
    "# transforms = transforms.Compose([transforms.Resize((32, 32)),\n",
    "#                                  transforms.ToTensor()])\n",
    "\n",
    "\n",
    "# train_dataset = mnist.MNIST(root='./train', train=True, transform=transforms,download=True)\n",
    "# test_dataset = mnist.MNIST(root='./test', train=False, transform=transforms,download=True)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=256,shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=256,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be6ce40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNetNormInput(nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes):\n",
    "        super(LeNetNormInput, self).__init__()\n",
    "        \n",
    "        self.feature_extractor = nn.Sequential(     \n",
    "        nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0),\n",
    "        nn.BatchNorm2d(6),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
    "        nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n",
    "        nn.BatchNorm2d(16),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        \n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "        nn.Linear(400, 120),\n",
    "        nn.BatchNorm1d(120),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(120, 84),\n",
    "        nn.BatchNorm1d(84),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(84, n_classes)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        logits = self.classifier(x)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        return logits, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c35a023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\tTrain loss: 0.2195\tValid loss: 0.0275\tTrain accuracy: 98.12\tValid accuracy: 98.08\n",
      "Epoch: 1\tTrain loss: 0.0576\tValid loss: 0.0183\tTrain accuracy: 98.87\tValid accuracy: 98.55\n",
      "Epoch: 2\tTrain loss: 0.0389\tValid loss: 0.0150\tTrain accuracy: 99.15\tValid accuracy: 98.81\n",
      "Epoch: 3\tTrain loss: 0.0291\tValid loss: 0.0131\tTrain accuracy: 99.39\tValid accuracy: 98.95\n",
      "Epoch: 4\tTrain loss: 0.0224\tValid loss: 0.0121\tTrain accuracy: 99.52\tValid accuracy: 98.99\n",
      "Epoch: 5\tTrain loss: 0.0174\tValid loss: 0.0116\tTrain accuracy: 99.58\tValid accuracy: 99.03\n",
      "Epoch: 6\tTrain loss: 0.0137\tValid loss: 0.0118\tTrain accuracy: 99.61\tValid accuracy: 99.02\n",
      "Epoch: 7\tTrain loss: 0.0109\tValid loss: 0.0118\tTrain accuracy: 99.69\tValid accuracy: 99.02\n",
      "Epoch: 8\tTrain loss: 0.0087\tValid loss: 0.0130\tTrain accuracy: 99.65\tValid accuracy: 98.96\n",
      "Epoch: 9\tTrain loss: 0.0071\tValid loss: 0.0123\tTrain accuracy: 99.74\tValid accuracy: 99.01\n"
     ]
    }
   ],
   "source": [
    "model = LeNetNormInput(10).to(DEVICE)\n",
    "optimizer = SGD(model.parameters(), lr=1e-1)\n",
    "criterion = CrossEntropyLoss()\n",
    "all_epoch = 10\n",
    "\n",
    "train_loss_arr=[]\n",
    "test_loss_arr=[]\n",
    "for current_epoch in range(all_epoch):\n",
    "    model.train()\n",
    "    loss_acc=0\n",
    "    for idx, (x_train, y_train) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        x_train=x_train.to(DEVICE)\n",
    "        y_train = y_train.to(DEVICE)\n",
    "        \n",
    "        #forward\n",
    "        y_hat_pred ,_= model(x_train)\n",
    "        loss = criterion(y_hat_pred, y_train)\n",
    "        loss_acc += loss.item() * x_train.size(0)\n",
    "\n",
    "              \n",
    "        #backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss_train = loss_acc / len(train_loader.dataset)\n",
    "        train_loss_arr.append(epoch_loss_train)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    all_correct_num = 0\n",
    "    all_sample_num = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        loss_acc=0\n",
    "        \n",
    "        for idx, (x_test, y_test) in enumerate(test_loader):\n",
    "            x_test=x_test.to(DEVICE)\n",
    "            y_test=y_test.to(DEVICE)\n",
    "            y_hat_pred ,_= model(x_test)\n",
    "            loss = criterion(y_hat_pred, y_test) \n",
    "            loss_acc += loss.item() * x_train.size(0)\n",
    "\n",
    "        epoch_loss_test = loss_acc / len(test_loader.dataset)    \n",
    "        test_loss_arr.append(epoch_loss_test)\n",
    "\n",
    "        train_acc = get_accuracy(model, train_loader, device=DEVICE)\n",
    "        valid_acc = get_accuracy(model, test_loader, device=DEVICE)\n",
    "                \n",
    "        print(f'Epoch: {current_epoch}\\t'\n",
    "            f'Train loss: {epoch_loss_train:.4f}\\t'\n",
    "            f'Valid loss: {epoch_loss_test:.4f}\\t'\n",
    "            f'Train accuracy: {100 * train_acc:.2f}\\t'\n",
    "            f'Valid accuracy: {100 * valid_acc:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91843c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#learned batch norm params example\n",
    "list(list(model.classifier.children())[1].parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96349c4d",
   "metadata": {},
   "source": [
    "3. Next instead of standard normalization use batch normalization for the input layer also and train the\n",
    "network. Plot the distribution of learned batch norm parameters for each layer (including input) using\n",
    "violin plots. Compare the train/test accuracy and loss for the two cases? Did batch normalization for\n",
    "the input layer improve performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86204ea8",
   "metadata": {},
   "source": [
    "Ans:<br>\n",
    "As seen below in the output the performance is about the same albeit negligibly better(which is in the realm of error of running model performance)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ccab1bc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#prepare data.\n",
    "\n",
    "transforms = transforms.Compose([transforms.Resize((32, 32)),\n",
    "                                 transforms.ToTensor()])\n",
    "\n",
    "\n",
    "train_dataset = mnist.MNIST(root='./train', train=True, transform=transforms,download=True)\n",
    "test_dataset = mnist.MNIST(root='./test', train=False, transform=transforms,download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=256,shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bc5fa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNetNormAll(nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes):\n",
    "        super(LeNetNormAll, self).__init__()\n",
    "        \n",
    "        self.feature_extractor = nn.Sequential(\n",
    "        nn.BatchNorm2d(1),            \n",
    "        nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0),\n",
    "        nn.BatchNorm2d(6),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
    "        nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n",
    "        nn.BatchNorm2d(16),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        \n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "        nn.Linear(400, 120),\n",
    "        nn.BatchNorm1d(120),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(120, 84),\n",
    "        nn.BatchNorm1d(84),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(84, n_classes)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        logits = self.classifier(x)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        return logits, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "521269f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\tTrain loss: 0.2244\tValid loss: 0.0291\tTrain accuracy: 98.25\tValid accuracy: 98.10\n",
      "Epoch: 1\tTrain loss: 0.0567\tValid loss: 0.0185\tTrain accuracy: 98.96\tValid accuracy: 98.55\n",
      "Epoch: 2\tTrain loss: 0.0396\tValid loss: 0.0199\tTrain accuracy: 98.77\tValid accuracy: 98.36\n",
      "Epoch: 3\tTrain loss: 0.0306\tValid loss: 0.0322\tTrain accuracy: 97.87\tValid accuracy: 97.44\n",
      "Epoch: 4\tTrain loss: 0.0237\tValid loss: 0.0143\tTrain accuracy: 99.21\tValid accuracy: 98.75\n",
      "Epoch: 5\tTrain loss: 0.0195\tValid loss: 0.0359\tTrain accuracy: 97.78\tValid accuracy: 96.93\n",
      "Epoch: 6\tTrain loss: 0.0172\tValid loss: 0.0178\tTrain accuracy: 99.22\tValid accuracy: 98.55\n",
      "Epoch: 7\tTrain loss: 0.0148\tValid loss: 0.0123\tTrain accuracy: 99.67\tValid accuracy: 98.97\n",
      "Epoch: 8\tTrain loss: 0.0122\tValid loss: 0.0115\tTrain accuracy: 99.84\tValid accuracy: 99.07\n",
      "Epoch: 9\tTrain loss: 0.0100\tValid loss: 0.0102\tTrain accuracy: 99.91\tValid accuracy: 99.21\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = LeNetNormAll(10).to(DEVICE)\n",
    "optimizer = SGD(model.parameters(), lr=1e-1)\n",
    "criterion = CrossEntropyLoss()\n",
    "all_epoch = 10\n",
    "\n",
    "train_loss_arr=[]\n",
    "test_loss_arr=[]\n",
    "for current_epoch in range(all_epoch):\n",
    "    model.train()\n",
    "    loss_acc=0\n",
    "    for idx, (x_train, y_train) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        x_train=x_train.to(DEVICE)\n",
    "        y_train = y_train.to(DEVICE)\n",
    "        \n",
    "        #forward\n",
    "        y_hat_pred ,_= model(x_train)\n",
    "        loss = criterion(y_hat_pred, y_train)\n",
    "        loss_acc += loss.item() * x_train.size(0)\n",
    "\n",
    "              \n",
    "        #backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss_train = loss_acc / len(train_loader.dataset)\n",
    "        train_loss_arr.append(epoch_loss_train)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    all_correct_num = 0\n",
    "    all_sample_num = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        loss_acc=0\n",
    "        \n",
    "        for idx, (x_test, y_test) in enumerate(test_loader):\n",
    "            x_test=x_test.to(DEVICE)\n",
    "            y_test=y_test.to(DEVICE)\n",
    "            y_hat_pred ,_= model(x_test)\n",
    "            loss = criterion(y_hat_pred, y_test) \n",
    "            loss_acc += loss.item() * x_train.size(0)\n",
    "\n",
    "        epoch_loss_test = loss_acc / len(test_loader.dataset)    \n",
    "        test_loss_arr.append(epoch_loss_test)\n",
    "\n",
    "        train_acc = get_accuracy(model, train_loader, device=DEVICE)\n",
    "        valid_acc = get_accuracy(model, test_loader, device=DEVICE)\n",
    "                \n",
    "        print(f'Epoch: {current_epoch}\\t'\n",
    "            f'Train loss: {epoch_loss_train:.4f}\\t'\n",
    "            f'Valid loss: {epoch_loss_test:.4f}\\t'\n",
    "            f'Train accuracy: {100 * train_acc:.2f}\\t'\n",
    "            f'Valid accuracy: {100 * valid_acc:.2f}')\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f01cad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.violinplot(list(list(model.classifier.children())[1].parameters())[0].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c21e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.violinplot(list(list(model.classifier.children())[1].parameters())[1].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f94e1eb",
   "metadata": {},
   "source": [
    "4. Train the network without batch normalization but this time use dropout. For hidden layers use a\n",
    "dropout probability of 0.5 and for input, layer take it to be 0.2 Compare test accuracy using dropout\n",
    "to test accuracy obtained using batch normalization in parts 2 and 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15824b08",
   "metadata": {},
   "source": [
    "ans:<br>\n",
    "As seen below in the output the performance is a little bit worse than the above 2 models but again pretty negligible.(about a percent difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5e0e4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNetDropOutNoBatch(nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes):\n",
    "        super(LeNetDropOutNoBatch, self).__init__()\n",
    "        \n",
    "        self.feature_extractor = nn.Sequential(            \n",
    "        nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0),\n",
    "        nn.Dropout(p=0.2),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
    "        nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n",
    "        nn.Dropout(p=0.5),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        \n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "        nn.Linear(400, 120),\n",
    "        nn.Dropout(p=0.5),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(120, 84),\n",
    "        nn.Dropout(p=0.5),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(84, n_classes)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        logits = self.classifier(x)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        return logits, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c24bcd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\tTrain loss: 1.2538\tValid loss: 0.1318\tTrain accuracy: 93.32\tValid accuracy: 93.67\n",
      "Epoch: 1\tTrain loss: 0.3491\tValid loss: 0.0792\tTrain accuracy: 95.96\tValid accuracy: 96.10\n",
      "Epoch: 2\tTrain loss: 0.2492\tValid loss: 0.0944\tTrain accuracy: 95.23\tValid accuracy: 95.27\n",
      "Epoch: 3\tTrain loss: 0.2069\tValid loss: 0.0491\tTrain accuracy: 97.21\tValid accuracy: 97.31\n",
      "Epoch: 4\tTrain loss: 0.1827\tValid loss: 0.0422\tTrain accuracy: 97.64\tValid accuracy: 97.76\n",
      "Epoch: 5\tTrain loss: 0.1643\tValid loss: 0.0398\tTrain accuracy: 97.82\tValid accuracy: 97.76\n",
      "Epoch: 6\tTrain loss: 0.1515\tValid loss: 0.0364\tTrain accuracy: 98.05\tValid accuracy: 97.90\n",
      "Epoch: 7\tTrain loss: 0.1384\tValid loss: 0.0377\tTrain accuracy: 98.11\tValid accuracy: 98.11\n",
      "Epoch: 8\tTrain loss: 0.1323\tValid loss: 0.0323\tTrain accuracy: 98.14\tValid accuracy: 98.13\n",
      "Epoch: 9\tTrain loss: 0.1278\tValid loss: 0.0268\tTrain accuracy: 98.41\tValid accuracy: 98.35\n"
     ]
    }
   ],
   "source": [
    "model = LeNetDropOutNoBatch(10).to(DEVICE)\n",
    "optimizer = SGD(model.parameters(), lr=1e-1)\n",
    "criterion = CrossEntropyLoss()\n",
    "all_epoch = 10\n",
    "\n",
    "train_loss_arr=[]\n",
    "test_loss_arr=[]\n",
    "for current_epoch in range(all_epoch):\n",
    "    model.train()\n",
    "    loss_acc=0\n",
    "    for idx, (x_train, y_train) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        x_train=x_train.to(DEVICE)\n",
    "        y_train = y_train.to(DEVICE)\n",
    "        \n",
    "        #forward\n",
    "        y_hat_pred ,_= model(x_train)\n",
    "        loss = criterion(y_hat_pred, y_train)\n",
    "        loss_acc += loss.item() * x_train.size(0)\n",
    "\n",
    "              \n",
    "        #backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss_train = loss_acc / len(train_loader.dataset)\n",
    "        train_loss_arr.append(epoch_loss_train)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    all_correct_num = 0\n",
    "    all_sample_num = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        loss_acc=0\n",
    "        \n",
    "        for idx, (x_test, y_test) in enumerate(test_loader):\n",
    "            x_test=x_test.to(DEVICE)\n",
    "            y_test=y_test.to(DEVICE)\n",
    "            y_hat_pred ,_= model(x_test)\n",
    "            loss = criterion(y_hat_pred, y_test) \n",
    "            loss_acc += loss.item() * x_train.size(0)\n",
    "\n",
    "        epoch_loss_test = loss_acc / len(test_loader.dataset)    \n",
    "        test_loss_arr.append(epoch_loss_test)\n",
    "\n",
    "        train_acc = get_accuracy(model, train_loader, device=DEVICE)\n",
    "        valid_acc = get_accuracy(model, test_loader, device=DEVICE)\n",
    "                \n",
    "        print(f'Epoch: {current_epoch}\\t'\n",
    "            f'Train loss: {epoch_loss_train:.4f}\\t'\n",
    "            f'Valid loss: {epoch_loss_test:.4f}\\t'\n",
    "            f'Train accuracy: {100 * train_acc:.2f}\\t'\n",
    "            f'Valid accuracy: {100 * valid_acc:.2f}')\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf10d22",
   "metadata": {},
   "source": [
    "5.Now train the network using both batch normalization and dropout. How does the performance (test\n",
    "accuracy) of the network compare with the cases with dropout alone and with batch normalization\n",
    "alone?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1396a07f",
   "metadata": {},
   "source": [
    "ans:<br>\n",
    "As seen below in the output the performance is a little bit worse than just using batch norm but similar to dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f051e0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNetDropOutBatch(nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes):\n",
    "        super(LeNetDropOutBatch, self).__init__()\n",
    "        \n",
    "        self.feature_extractor = nn.Sequential(            \n",
    "        nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0),\n",
    "        nn.BatchNorm2d(6),\n",
    "        nn.Dropout(p=0.2),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
    "        nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n",
    "        nn.BatchNorm2d(16),\n",
    "        nn.Dropout(p=0.5),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        \n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "        nn.Linear(400, 120),\n",
    "        nn.BatchNorm1d(120),\n",
    "        nn.Dropout(p=0.5),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(120, 84),\n",
    "        nn.BatchNorm1d(84),\n",
    "        nn.Dropout(p=0.5),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(84, n_classes)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        logits = self.classifier(x)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        return logits, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "efda82e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\tTrain loss: 0.6396\tValid loss: 0.1017\tTrain accuracy: 94.70\tValid accuracy: 95.56\n",
      "Epoch: 1\tTrain loss: 0.2359\tValid loss: 0.0660\tTrain accuracy: 95.98\tValid accuracy: 96.48\n",
      "Epoch: 2\tTrain loss: 0.1752\tValid loss: 0.0568\tTrain accuracy: 96.20\tValid accuracy: 96.47\n",
      "Epoch: 3\tTrain loss: 0.1501\tValid loss: 0.0452\tTrain accuracy: 96.94\tValid accuracy: 97.22\n",
      "Epoch: 4\tTrain loss: 0.1327\tValid loss: 0.0381\tTrain accuracy: 97.37\tValid accuracy: 97.64\n",
      "Epoch: 5\tTrain loss: 0.1197\tValid loss: 0.0359\tTrain accuracy: 97.57\tValid accuracy: 97.79\n",
      "Epoch: 6\tTrain loss: 0.1092\tValid loss: 0.0297\tTrain accuracy: 98.01\tValid accuracy: 98.08\n",
      "Epoch: 7\tTrain loss: 0.1047\tValid loss: 0.0319\tTrain accuracy: 97.83\tValid accuracy: 98.01\n",
      "Epoch: 8\tTrain loss: 0.0959\tValid loss: 0.0255\tTrain accuracy: 98.29\tValid accuracy: 98.46\n",
      "Epoch: 9\tTrain loss: 0.0945\tValid loss: 0.0295\tTrain accuracy: 97.92\tValid accuracy: 98.02\n"
     ]
    }
   ],
   "source": [
    "model = LeNetDropOutBatch(10).to(DEVICE)\n",
    "optimizer = SGD(model.parameters(), lr=1e-1)\n",
    "criterion = CrossEntropyLoss()\n",
    "all_epoch = 10\n",
    "\n",
    "train_loss_arr=[]\n",
    "test_loss_arr=[]\n",
    "for current_epoch in range(all_epoch):\n",
    "    model.train()\n",
    "    loss_acc=0\n",
    "    for idx, (x_train, y_train) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        x_train=x_train.to(DEVICE)\n",
    "        y_train = y_train.to(DEVICE)\n",
    "        \n",
    "        #forward\n",
    "        y_hat_pred ,_= model(x_train)\n",
    "        loss = criterion(y_hat_pred, y_train)\n",
    "        loss_acc += loss.item() * x_train.size(0)\n",
    "\n",
    "              \n",
    "        #backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss_train = loss_acc / len(train_loader.dataset)\n",
    "        train_loss_arr.append(epoch_loss_train)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    all_correct_num = 0\n",
    "    all_sample_num = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        loss_acc=0\n",
    "        \n",
    "        for idx, (x_test, y_test) in enumerate(test_loader):\n",
    "            x_test=x_test.to(DEVICE)\n",
    "            y_test=y_test.to(DEVICE)\n",
    "            y_hat_pred ,_= model(x_test)\n",
    "            loss = criterion(y_hat_pred, y_test) \n",
    "            loss_acc += loss.item() * x_train.size(0)\n",
    "\n",
    "        epoch_loss_test = loss_acc / len(test_loader.dataset)    \n",
    "        test_loss_arr.append(epoch_loss_test)\n",
    "\n",
    "        train_acc = get_accuracy(model, train_loader, device=DEVICE)\n",
    "        valid_acc = get_accuracy(model, test_loader, device=DEVICE)\n",
    "                \n",
    "        print(f'Epoch: {current_epoch}\\t'\n",
    "            f'Train loss: {epoch_loss_train:.4f}\\t'\n",
    "            f'Valid loss: {epoch_loss_test:.4f}\\t'\n",
    "            f'Train accuracy: {100 * train_acc:.2f}\\t'\n",
    "            f'Valid accuracy: {100 * valid_acc:.2f}')\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f6e25d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69ad49e8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bdc79dd9",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e06fa6",
   "metadata": {},
   "source": [
    "1. Calculate the number of parameters in Alexnet. You will have to show calculations for each layer and\n",
    "then sum it to obtain the total number of parameters in Alexnet. When calculating you will need to\n",
    "account for all the filters (size, strides, padding) at each layer. Look at Sec. 3.5 and Figure 2 in Alexnet\n",
    "paper (see reference). Points will only be given when explicit calculations are shown for each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e6d6c3",
   "metadata": {},
   "source": [
    "Ans:\n",
    "O = Size (width) of output image.<br>\n",
    "I = Size (width) of input image.<br>\n",
    "K = Size (width) of kernels used in the Conv Layer.<br>\n",
    "N = Number of kernels.<br>\n",
    "S = Stride of the convolution operation.<br>\n",
    "P = Padding.<br>\n",
    "\n",
    "For conv layer output,<br>\n",
    "Output = $\\frac{I-  K + 2P}{S} + 1$ <br>\n",
    "for Maxpool,<br>\n",
    "Output = (I-Pool size)/S +1<br>\n",
    "\n",
    "\n",
    "For conv layer and fully connected layer,\n",
    "W_c = Number of weights of the Conv Layer.<br>\n",
    "B_c = Number of biases of the Conv Layer.<br>\n",
    "P_c = Number of parameters of the Conv Layer.<br>\n",
    "K = Size (width) of kernels used in the Conv Layer.<br>\n",
    "N = Number of kernels.<br>\n",
    "C = Number of channels of the input image.<br>\n",
    "\n",
    "\\begin{align*}W_c &= K^2 \\times C \\times N \\\\B_c &= N \\\\P_c &= W_c + B_c\\end{align*}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98870a13",
   "metadata": {},
   "source": [
    "Input = (227 -  11 + 2(0))/4 + 1 = 55 <br>\n",
    "conv1: (11)^2(3)(96)+96 = 34944 parameters <br>\n",
    "Output = (55 - 3)/2+1=27 <br>\n",
    "pool1: 0 parameters; size: 27 x 27 x 96 <br>\n",
    "Output = (27 -  5 + 2(2))/1 + 1 = 27 <br>\n",
    "conv2: (5)^2(96)(256)+256=614656 parameters <br>\n",
    "Output = (27 - 3)/2+1=13<br>\n",
    "pool2: 0 parameters; size: 13 x 13 x 256 <br>\n",
    "Output = (13 -  3 + 2(1))/1 + 1 = 13 <br>\n",
    "conv3: (3)^2(256)(384)+384=885120 parameters <br>\n",
    "Output = (13 -  3 + 2(1))/1 + 1 = 13<br>\n",
    "conv4: 3)^2(384)(384)+384=1327488 parameters<br>\n",
    "Output = (13 -  3 + 2(1))/1 + 1 = 13 <br>\n",
    "conv5: 3)^2(384)(256)+256=884992 parameters<br>\n",
    "Output = (13 - 3)/2+1=6 <br>\n",
    "pool5: 0 parameters; size: 6 x 6 x 256 <br>\n",
    "FC: (1)^2(9216)(4096)+4096=37752832 parameters<br>\n",
    "FC: (1)^2(4096)(4096)+4096=16781312 parameters<br>\n",
    "FC: (1)^2(4096)(1000)+1000=4097000 parameters<br>\n",
    "If we sum all the above parameters , we get the total,\n",
    "Total: 62,378,344 parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba8dc2e",
   "metadata": {},
   "source": [
    "2. VGG (Simonyan et al.) has an extremely homogeneous architecture that only performs 3x3 convolutions\n",
    "with stride 1 and pad 1 and 2x2 max pooling with stride 2 (and no padding) from the beginning to\n",
    "the end. However VGGNet is very expensive to evaluate and uses a lot more memory and parameters.\n",
    "Refer to VGG19 architecture on page 3 in Table 1 of the paper by Simonyan et al. You need to complete\n",
    "Table 1 below for calculating activation units and parameters at each layer in VGG19 (without counting\n",
    "biases). Its been partially filled for you. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d806f0",
   "metadata": {},
   "source": [
    "Ans:\n",
    "\n",
    "INPUT: [224x224x3]                memory:  224*224*3=150K                    parameter(compute): 0<br>\n",
    "CONV3-64: [224x224x64]          memory:  224*224*64=3.2M                    parameter(compute): (3*3*3)*64 = 1,728<br>\n",
    "CONV3-64: [224x224x64]          memory:  224*224*64=3.2M                    parameter(compute): (3*3*64)*64 = 36,864<br>\n",
    "POOL2: [112x112x64]          memory:  112*112*64=800K                        parameter(compute): 0<br>\n",
    "CONV3-128: [112x112x128        ]  memory:  112*112*128=1.6M                    parameter(compute): (3*3*64)*128 = 73,728<br>\n",
    "CONV3-128: [112x112x128        ]  memory:  112*112*128=1.6M                    parameter(compute): (3*3*128)*128 = 147,456<br>\n",
    "POOL2: [56x56x128]          memory:  56*56*128=400K                            parameter(compute): 0<br>\n",
    "CONV3-256: [56x56x256]          memory:  56*56*256=800K                         parameter(compute): (3*3*128)*256 = 294,912<br>\n",
    "CONV3-256: [56x56x256]          memory:  56*56*256=800K                         parameter(compute): (3*3*256)*256 = 589,824<br>\n",
    "CONV3-256: [56x56x256]          memory:  56*56*256=800K                        parameter(compute): (3*3*256)*256 = 589,824<br>\n",
    "POOL2: [28x28x256]          memory:  28*28*256=200K                            parameter(compute): 0<br>\n",
    "CONV3-512: [28x28x512]          memory:  28*28*512=400K                        parameter(compute): (3*3*256)*512 = 1,179,648<br>\n",
    "CONV3-512: [28x28x512]          memory:  28*28*512=400K                        parameter(compute): (3*3*512)*512 = 2,359,296<br>\n",
    "CONV3-512: [28x28x512]          memory:  28*28*512=400K                       parameter(compute): (3*3*512)*512 = 2,359,296<br>\n",
    "POOL2: [14x14x512]          memory:  14*14*512=100K                             parameter(compute): 0<br>\n",
    "CONV3-512: [14x14x512]          memory:  14*14*512=100K                     parameter(compute): (3*3*512)*512 = 2,359,296<br>\n",
    "CONV3-512: [14x14x512]          memory:  14*14*512=100K                         parameter(compute): (3*3*512)*512 = 2,359,296<br>\n",
    "CONV3-512: [14x14x512]          memory:  14*14*512=100K                         parameter(compute): (3*3*512)*512 = 2,359,296<br>\n",
    "POOL2: [7x7x512]            memory:  7*7*512=25K                                  parameter(compute): 0<br>\n",
    "FC: [1x1x4096]          memory:  4096                                          parameter(compute): 7*7*512*4096 = 102,760,448<br>\n",
    "FC: [1x1x4096]          memory:  4096                                       parameter(compute): 4096*4096 = 16,777,216<br>\n",
    "FC: [1x1x1000]          memory:  1000                                          parameter(compute): 4096*1000 = 4,096,000<br>\n",
    "\n",
    "total          memory: 24M (approx)                                                     params: 138M parameters<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f16fe6b",
   "metadata": {},
   "source": [
    "3. VGG architectures have smaller filters but deeper networks compared to Alexnet (3x3 compared to\n",
    "11x11 or 5x5). Show that a stack of N convolution layers each of filter size F × F has the same\n",
    "receptive field as one convolution layer with filter of size (NF − N + 1) × (NF − N + 1). Use this to\n",
    "calculate the receptive field of 3 filters of size 5x5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4e21a6",
   "metadata": {},
   "source": [
    "If we consider a stack of N layers of convolution of filter size F*F:<br>\n",
    "Shape of kernel(receptive field)= S(shape)-F+1,<br>\n",
    "For N layers, this becomes = S-N*(F+1)<br>\n",
    "S-NF-N<br>\n",
    "<br>\n",
    "For one layer with filter (NF − N + 1)<br>\n",
    "receptive field = S-(NF − N + 1)+1 = S-NF-N<br>\n",
    "<br>\n",
    "which is the same.<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773413bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd31af7d",
   "metadata": {},
   "source": [
    "4. The original Googlenet paper (Szegedy et al.) proposes two architectures for Inception module, shown\n",
    "in Figure 2 on page 5 of the paper, referred to as naive and dimensionality reduction respectively.<br><br>\n",
    "(a) What is the general idea behind designing an inception module (parallel convolutional filters of\n",
    "different sizes with a pooling followed by concatenation) in a convolutional neural network ? (2)<br>\n",
    "(b) Assuming the input to inception module (referred to as ”previous layer” in Figure 2 of the paper) has size 32x32x256, calculate the output size after filter concatenation for the naive and\n",
    "dimensionality reduction inception architectures with number of filters given in Figure 1. (3)<br>\n",
    "(c) Next calculate the total number of convolutional operations for each of the two inception architecture again assuming the input to the module has dimensions 32x32x256 and number of filters\n",
    "given in Figure 1. (3)<br>\n",
    "(d) Based on the calculations in part (c) explain the problem with naive architecture and how dimensionality reduction architecture helps (Hint: compare computational complexity). How much is the\n",
    "computational saving ? (2+2)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd153d9",
   "metadata": {},
   "source": [
    "Ans 4a.<br>\n",
    "In the inception module instead of using sparse matrices in the kernels for more computations , we convert them to a more dense format . This can effectively take advantage of more efficient in-built matrix-multiplication routines. The idea is to make computations more efficient by increasing the effective network size.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7148ffd5",
   "metadata": {},
   "source": [
    "Ans 4b:<br>\n",
    "Output size after filter concatenation,<br>\n",
    "For the Naive: 32* 32 *(128+192+96+256) =688128<br>\n",
    "For Dimensinality Reduction inception architecture: 32 * 32 * (128+192+96+64) =491520\n",
    "So we can see that for the dimentionality reduction arch. the output size is smaller.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f22104",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17dd2578",
   "metadata": {},
   "source": [
    "Ans 4c:<br>\n",
    "Total no of Convolution ops:<br>\n",
    "<br>\n",
    "For the Naive implementation: <br>\n",
    "Conv1 = 32 * 32 * 1 * 256 * 128 = 33554432 <br>\n",
    "Conv3 = 32 * 32 * 9 * 256 * 128 = 301989888 <br>\n",
    "Conv5 = 32 * 32 * 25 * 256 * 128 = 838860800 <br>\n",
    "Total = 1174405120 <br>\n",
    "For Dimensinality Reduction inception architecture: <br>\n",
    "Conv1 = (32 * 32 * 256 * 128) + (32 * 32 * 256 * 128) + (32 * 32 * 256 * 64) = 92274688 <br>\n",
    "Conv3 = 32 * 32 * 9 * 128 * 192 = 226492416 <br>\n",
    "Conv5 = 32 * 32 * 25 * 32 * 96 = 78643200 <br>\n",
    "Total = 397410304 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28e7aea",
   "metadata": {},
   "source": [
    "Ans 4d:<br>\n",
    "We can easily see that the total for the naive implementation vs the total for the dimensionality reduction arch is:<br>\n",
    "1174405120 / 397410304= 2.95514511873"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc68f3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
